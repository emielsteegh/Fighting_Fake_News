{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\emiel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\emiel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\emiel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "sia = SIA()\n",
    "\n",
    "import gensim\n",
    "\n",
    "import nltk\n",
    "#file for punkt splitter\n",
    "nltk.download('punkt');\n",
    "#file for vader sentiment\n",
    "nltk.download('vader_lexicon');\n",
    "\n",
    "#wordnet lemmatization\n",
    "nltk.download('wordnet')\n",
    "#more for preprocessing\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"]=20,20\n",
    "%matplotlib inline\n",
    "\n",
    "import pickle\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 1: Load the dataset <a class=\"anchor\" id=\"chapter1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As U.S. budget fight looms, Republicans flip t...</td>\n",
       "      <td>WASHINGTON (Reuters) - The head of a conservat...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. military to accept transgender recruits o...</td>\n",
       "      <td>WASHINGTON (Reuters) - Transgender people will...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>\n",
       "      <td>WASHINGTON (Reuters) - The special counsel inv...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FBI Russia probe helped by Australian diplomat...</td>\n",
       "      <td>WASHINGTON (Reuters) - Trump campaign adviser ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 30, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump wants Postal Service to charge 'much mor...</td>\n",
       "      <td>SEATTLE/WASHINGTON (Reuters) - President Donal...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  As U.S. budget fight looms, Republicans flip t...   \n",
       "1  U.S. military to accept transgender recruits o...   \n",
       "2  Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
       "3  FBI Russia probe helped by Australian diplomat...   \n",
       "4  Trump wants Postal Service to charge 'much mor...   \n",
       "\n",
       "                                                text       subject  \\\n",
       "0  WASHINGTON (Reuters) - The head of a conservat...  politicsNews   \n",
       "1  WASHINGTON (Reuters) - Transgender people will...  politicsNews   \n",
       "2  WASHINGTON (Reuters) - The special counsel inv...  politicsNews   \n",
       "3  WASHINGTON (Reuters) - Trump campaign adviser ...  politicsNews   \n",
       "4  SEATTLE/WASHINGTON (Reuters) - President Donal...  politicsNews   \n",
       "\n",
       "                 date  \n",
       "0  December 31, 2017   \n",
       "1  December 29, 2017   \n",
       "2  December 31, 2017   \n",
       "3  December 30, 2017   \n",
       "4  December 29, 2017   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
       "\n",
       "                                                text subject  \\\n",
       "0  Donald Trump just couldn t wish all Americans ...    News   \n",
       "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
       "2  On Friday, it was revealed that former Milwauk...    News   \n",
       "3  On Christmas day, Donald Trump announced that ...    News   \n",
       "4  Pope Francis used his annual Christmas Day mes...    News   \n",
       "\n",
       "                date  \n",
       "0  December 31, 2017  \n",
       "1  December 31, 2017  \n",
       "2  December 30, 2017  \n",
       "3  December 29, 2017  \n",
       "4  December 25, 2017  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_path_true = os.path.join(\"sources\", \"ISOT\", \"True.csv\")\n",
    "dataset_path_fake = os.path.join(\"sources\", \"ISOT\", \"Fake.csv\")\n",
    "\n",
    "df_true = pd.read_csv(dataset_path_true, encoding='utf-8') # make sure to use the right encoding\n",
    "df_fake = pd.read_csv(dataset_path_fake, encoding='utf-8') \n",
    "\n",
    "dfm_true = df_true.head()\n",
    "dfm_fake = df_fake.head()\n",
    "\n",
    "display(dfm_true)\n",
    "display(dfm_fake)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2: Split the text into sentences <a class=\"anchor\" id=\"chapter2\"></a>\n",
    "\n",
    "in lab 03 we used a big manual function, but for now we will use the nltk tokenizer here :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     WASHINGTON (Reuters) - The head of a conservat...\n",
       "1     In keeping with a sharp pivot under way among ...\n",
       "28    The package far exceeded the $44 billion reque...\n",
       "29             The Senate has not yet voted on the aid.\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def split_sentences(article_text):\n",
    "    \"\"\"Takes a string, returns a list of its individual sentences ()\"\"\"\n",
    "    return pd.Series(nltk.tokenize.sent_tokenize(article_text))\n",
    "\n",
    "sample_sentences = split_sentences(dfm_true.text[0])\n",
    "display(sample_sentences.iloc[[0,1,-2,-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Corpus of Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_sentences = [] # a list of all documents (by sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Corpus of Entire Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amount of documents in corpus:  44898\n"
     ]
    }
   ],
   "source": [
    "corpus_texts = [] # list of all documents (by entire body)\n",
    "\n",
    "#corpus_texts = df_true['text'].tolist() + df_fake['text'].tolist() \n",
    "corpus_texts = df_true['text'].tolist() + df_fake['text'].tolist() \n",
    "\n",
    "print(\"amount of documents in corpus: \", len(corpus_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 3 Text Preprocessing\n",
    "In order to do proper topic analysis the text needs to become understandable by removing unineteresting properties of the text. We lower case it, stem and lemmatize it, and remove all words under 3 characters or stopwords (it them ...).\n",
    "\n",
    "Now with bi and tri grams!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_word_stream = [doc.split(\" \") for doc in corpus_texts]\n",
    "\n",
    "bigram = gensim.models.Phrases(corpus_word_stream, min_count=5, threshold=10) # 5 and 10 are the default values, but this can be tweaked\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=10)\n",
    "\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_lemmatize(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['follow', 'statement', 'post', 'verifi', 'twitter', 'account', 'presid', 'donald', 'trump', 'realdonaldtrump', 'potu', 'opinion', 'express', 'reuter', 'edit', 'statement', 'confirm', 'accuraci', 'realdonaldtrump', 'fake', 'news', 'love', 'talk', 'call', 'approv', 'rat', 'foxandfriend', 'show', 'rat', 'approxim', 'presid', 'obama', 'despit', 'massiv', 'neg', 'trump', 'coverag', 'russia', 'hoax', 'unit', 'state', 'post', 'offic', 'lose', 'billion', 'dollar', 'year', 'charg', 'amazon', 'littl', 'deliv', 'packag', 'make', 'amazon', 'richer', 'post', 'offic', 'dumber', 'poorer', 'charg', 'sourc', 'link', 'jpexyr']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def preprocess(text):\n",
    "    output = []\n",
    "    \n",
    "    #remove stopwords+simpleprepro\n",
    "    #bi/tri-gram building\n",
    "    #lemma\n",
    "    # TODO where goes stemming?\n",
    "    \n",
    "    \n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            output.append(stem_lemmatize(token))\n",
    "    return output\n",
    "\n",
    "print(preprocess(corpus_texts[7]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 4 Dictionary Creation\n",
    "\n",
    "word embeddings, all occuring words are stored and get a number (embedding) those embeddings can later be used for vector calculations. of course not all words are important, so words that appear more than 100000 times or that exist in >60%  our corpus (those are very likely words slipped the stopword list) and less than 15 time (words not important enought for a topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unfiltered:  Dictionary(83579 unique tokens: ['action', 'administr', 'agre', 'aid', 'approach']...)\n",
      "  filtered:  Dictionary(13703 unique tokens: ['action', 'administr', 'agre', 'aid', 'approach']...)\n",
      "Wall time: 6min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Creating the gensim dictionary of word embeddings\n",
    "\n",
    "if (0):\n",
    "    dictionary = gensim.corpora.Dictionary.load(os.path.join(\"gensim\", \"dictionary\"))\n",
    "\n",
    "\n",
    "if (1):\n",
    "    \n",
    "    processed_corpus_texts = [preprocess(d) for d in corpus_texts]\n",
    "\n",
    "    dictionary = gensim.corpora.Dictionary(processed_corpus_texts)\n",
    "    print(\"unfiltered: \", dictionary)\n",
    "\n",
    "    dictionary.filter_extremes(no_below=15, no_above=0.6, keep_n=100000)\n",
    "    \n",
    "print(\"  filtered: \", dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 5 Bag of Words creation\n",
    "Now we create a vector representation in the form of a bag of words for eacht document. a vector that lets us know how often each word in the preprocessed text, that also exists in our dictionary, occurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if (1):\n",
    "    with open('bag_of_words.pickle', 'rb') as f: dictionary = pickle.load(f)\n",
    "\n",
    "if (0):\n",
    "    bow_corpus_texts = [dictionary.doc2bow(text) for text in processed_corpus_texts]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 6 LDA model creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "num_topics = 15\n",
    "if (0):\n",
    "    lda_model = gensim.models.LdaMulticore.load(os.path.join(\"gensim\", \"lda_model20\"))\n",
    "if (1):\n",
    "    lda_modelX = gensim.models.LdaMulticore(corpus = bow_corpus_texts,\n",
    "                                            id2word = dictionary,\n",
    "                                            num_topics = num_topics,\n",
    "                                            passes = 20,\n",
    "                                            iterations = 200,\n",
    "                                            eta = 'auto',\n",
    "                                            #alpha = 'auto',\n",
    "                                            workers = 2)\n",
    "    # multicore speeds up the process significantly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average topic coherence: -1.5653.\n",
      "[([(0.015163837, 'peopl'), (0.009979308, 'american'), (0.009363801, 'black'), (0.009183135, 'right'), (0.008101126, 'like'), (0.0076126237, 'america'), (0.00614769, 'live'), (0.006065929, 'student'), (0.0058811433, 'school'), (0.005828118, 'white'), (0.00544905, 'support'), (0.00516922, 'want'), (0.004894576, 'nation'), (0.004520604, 'think'), (0.004476691, 'http'), (0.004425941, 'women'), (0.004354366, 'countri'), (0.0041992464, 'go'), (0.004044317, 'year'), (0.004040936, 'come')], -1.2379890213623088), ([(0.012892715, 'news'), (0.011615788, 'like'), (0.011436488, 'time'), (0.011379721, 'know'), (0.011010983, 'imag'), (0.010515623, 'go'), (0.00945052, 'media'), (0.008276315, 'video'), (0.008199434, 'twitter'), (0.008135565, 'think'), (0.007475627, 'fact'), (0.0072948793, 'watch'), (0.0071493187, 'peopl'), (0.007148303, 'claim'), (0.0067389724, 'featur'), (0.006581249, 'thing'), (0.0062599415, 'actual'), (0.006170679, 'fake'), (0.006101773, 'hillari'), (0.0060443194, 'come')], -1.3444379628401788), ([(0.023017047, 'north'), (0.0205672, 'china'), (0.018596465, 'state'), (0.018130329, 'korea'), (0.017773563, 'nuclear'), (0.014905182, 'iran'), (0.014786682, 'unit'), (0.009824712, 'militari'), (0.00921693, 'missil'), (0.008589738, 'sanction'), (0.00828594, 'south'), (0.00821328, 'deal'), (0.008194712, 'offici'), (0.007684513, 'countri'), (0.0071878536, 'reuter'), (0.007161966, 'foreign'), (0.0067310757, 'defens'), (0.0066479673, 'chines'), (0.006502288, 'korean'), (0.0063856267, 'washington')], -1.3513339656616647), ([(0.020600818, 'russian'), (0.020130867, 'clinton'), (0.019779379, 'russia'), (0.015256098, 'investig'), (0.01396208, 'email'), (0.012963138, 'intellig'), (0.011959906, 'report'), (0.011929117, 'elect'), (0.009677683, 'inform'), (0.008328233, 'offici'), (0.007796712, 'campaign'), (0.0077133244, 'hack'), (0.0071139503, 'putin'), (0.0070585776, 'agenc'), (0.0067418455, 'hillari'), (0.0063235564, 'state'), (0.0062667904, 'secur'), (0.0061047827, 'comey'), (0.0060038194, 'committe'), (0.0057588024, 'news')], -1.3726060089691674), ([(0.015378921, 'syria'), (0.011764285, 'state'), (0.010536225, 'wire'), (0.009157817, 'militari'), (0.008953986, 'govern'), (0.007393809, 'centuri'), (0.006925835, 'syrian'), (0.0068614865, 'forc'), (0.0064531337, 'oper'), (0.006266902, 'terrorist'), (0.006110739, 'group'), (0.0060504545, 'isi'), (0.0057034786, 'attack'), (0.005320882, 'iraq'), (0.0053057033, 'media'), (0.0045193187, 'american'), (0.004405127, 'washington'), (0.004079045, 'support'), (0.0040495307, 'world'), (0.0039446442, 'member')], -1.388991408697995), ([(0.02609708, 'court'), (0.020139486, 'state'), (0.013105757, 'order'), (0.013081927, 'feder'), (0.012579224, 'justic'), (0.012229453, 'case'), (0.011363138, 'judg'), (0.011275734, 'rule'), (0.010362408, 'depart'), (0.0102836825, 'immigr'), (0.010015101, 'legal'), (0.009618519, 'govern'), (0.008674173, 'attorney'), (0.00853877, 'illeg'), (0.007246397, 'execut'), (0.00677588, 'right'), (0.0062567177, 'suprem'), (0.00613094, 'administr'), (0.006067996, 'gener'), (0.005857327, 'issu')], -1.4200199892166425), ([(0.061218135, 'trump'), (0.0391484, 'clinton'), (0.02357854, 'campaign'), (0.021890786, 'hillari'), (0.017511105, 'candid'), (0.017434081, 'elect'), (0.01564626, 'republican'), (0.014058205, 'donald'), (0.0138928015, 'presidenti'), (0.013396164, 'support'), (0.012251833, 'parti'), (0.0114406375, 'democrat'), (0.010953311, 'sander'), (0.009842442, 'cruz'), (0.008590084, 'state'), (0.008583912, 'polit'), (0.007979059, 'poll'), (0.0077593657, 'voter'), (0.007754309, 'rubio'), (0.0060036415, 'vote')], -1.441589311300677), ([(0.013825002, 'million'), (0.013134247, 'year'), (0.010514264, 'fund'), (0.010112243, 'compani'), (0.008862965, 'percent'), (0.008707821, 'money'), (0.00861726, 'billion'), (0.0077792555, 'govern'), (0.0076252157, 'bank'), (0.0075400756, 'busi'), (0.0067638536, 'financi'), (0.006173988, 'state'), (0.0060983174, 'market'), (0.0059832693, 'spend'), (0.0059474898, 'plan'), (0.0058886055, 'american'), (0.005771227, 'work'), (0.0055823238, 'program'), (0.005475355, 'cost'), (0.00534862, 'dollar')], -1.4817199449013023), ([(0.12410082, 'trump'), (0.0529297, 'presid'), (0.031076597, 'obama'), (0.023528941, 'donald'), (0.019143453, 'white'), (0.016199013, 'hous'), (0.0075550447, 'administr'), (0.005786873, 'american'), (0.0057470775, 'twitter'), (0.0057242955, 'tell'), (0.005409608, 'offic'), (0.005333165, 'barack'), (0.0052652247, 'go'), (0.0049630664, 'meet'), (0.0048053814, 'take'), (0.0047441213, 'washington'), (0.0045725605, 'realdonaldtrump'), (0.004438061, 'tweet'), (0.0044139805, 'januari'), (0.004132381, 'think')], -1.4898202171244832), ([(0.03662641, 'republican'), (0.02888047, 'senat'), (0.028130991, 'vote'), (0.027678441, 'democrat'), (0.015380776, 'elect'), (0.01347155, 'hous'), (0.012643949, 'state'), (0.011607553, 'parti'), (0.008656711, 'presid'), (0.00836748, 'congress'), (0.008026107, 'voter'), (0.0075795962, 'obama'), (0.007033638, 'legisl'), (0.006387701, 'ryan'), (0.005945872, 'committe'), (0.005917384, 'repres'), (0.005827366, 'support'), (0.005808132, 'year'), (0.0054658717, 'pass'), (0.005325837, 'mccain')], -1.5046606184815592), ([(0.025252149, 'polic'), (0.013419771, 'offic'), (0.011807571, 'year'), (0.010251715, 'report'), (0.009250911, 'citi'), (0.008501854, 'protest'), (0.008361337, 'kill'), (0.00747577, 'shoot'), (0.007257508, 'tell'), (0.006972546, 'arrest'), (0.0061777253, 'attack'), (0.0060841152, 'death'), (0.006059296, 'crime'), (0.005830535, 'charg'), (0.0055945967, 'accord'), (0.0055460315, 'famili'), (0.0051823375, 'investig'), (0.0051767607, 'peopl'), (0.0051352, 'take'), (0.004906956, 'incid')], -1.5237824120769052), ([(0.023633556, 'muslim'), (0.017882252, 'refuge'), (0.016489299, 'state'), (0.015435527, 'countri'), (0.014278478, 'israel'), (0.0126070045, 'secur'), (0.012082188, 'border'), (0.012009312, 'unit'), (0.009846511, 'peopl'), (0.009355243, 'islam'), (0.008957292, 'nation'), (0.008208124, 'immigr'), (0.007235453, 'isra'), (0.0071582412, 'attack'), (0.006971619, 'group'), (0.0058032763, 'palestinian'), (0.005673233, 'reuter'), (0.0055767014, 'year'), (0.005551349, 'migrant'), (0.005420179, 'govern')], -1.5670285154286256), ([(0.014202176, 'govern'), (0.013134418, 'minist'), (0.012939092, 'parti'), (0.009568944, 'reuter'), (0.008556347, 'european'), (0.008365154, 'leader'), (0.0075300536, 'presid'), (0.007419594, 'elect'), (0.006883359, 'saudi'), (0.0067647602, 'prime'), (0.006726523, 'polit'), (0.006624193, 'countri'), (0.006568413, 'region'), (0.0065665883, 'year'), (0.0059736776, 'britain'), (0.0056162095, 'germani'), (0.0055210823, 'talk'), (0.005356864, 'turkey'), (0.005287461, 'union'), (0.0052843546, 'state')], -1.6306459832632276), ([(0.0202242, 'media'), (0.020165607, 'wire'), (0.01274632, 'shoot'), (0.012346323, 'stori'), (0.009230635, 'news'), (0.008973376, 'event'), (0.008790587, 'centuri'), (0.008614575, 'room'), (0.008348345, 'polit'), (0.0071505792, 'radio'), (0.007060938, 'live'), (0.006947688, 'boiler'), (0.0064814626, 'join'), (0.0062857955, 'shooter'), (0.006173391, 'broadcast'), (0.005945546, 'facebook'), (0.005866341, 'week'), (0.0058450405, 'social'), (0.005523906, 'mass'), (0.0053550582, 'link')], -2.1558555276137694), ([(0.015335417, 'water'), (0.013500111, 'climat'), (0.012826292, 'health'), (0.011573391, 'abort'), (0.009949439, 'chang'), (0.009908801, 'state'), (0.008825377, 'plan'), (0.007245732, 'medic'), (0.006530764, 'energi'), (0.0058277193, 'parenthood'), (0.0055336715, 'power'), (0.005138106, 'environment'), (0.005012569, 'island'), (0.005010523, 'global'), (0.0047150366, 'peopl'), (0.0047060144, 'nation'), (0.004595773, 'puerto'), (0.0044455808, 'hurrican'), (0.0043948977, 'hospit'), (0.004177081, 'coal')], -2.5686103220961862)]\n"
     ]
    }
   ],
   "source": [
    "top_topics = lda_modelX.top_topics(bow_corpus_texts)\n",
    "\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / 15\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "print(top_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 7 TF-IDF x LDA model creation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "tfidf_model = gensim.models.TfidfModel(bow_corpus_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 8 Case Examination\n",
    "take a look at some classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO LDAVIS For TWEAKING SHIT SAVE PICTURES AND STEPS WHOOO\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "visualisation = pyLDAvis.gensim.prepare(lda_model50, bow_corpus_texts, dictionary)\n",
    "pyLDAvis.save_html(visualisation, os.path.join(\"gensim\", \"LDAvis\", \"LDAvis.hml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "most frequently occuring word:  trump\n",
      "\n",
      "\n",
      "tf-idf representation:\n",
      "[(54, 0.05205938794681232), (55, 0.05354190205365242), (61, 0.07113221361508652), (75, 0.03997404751583847), (79, 0.03459992716929177), (97, 0.07092158048519583), (98, 0.06297481560343694), (118, 0.04558896010987835), (120, 0.035550290840993065), (121, 0.04885654036858967), (137, 0.05105277762175444), (151, 0.01199919208264615), (158, 0.04117145008710115), (188, 0.05748465301769279), (189, 0.0648661753742098), (207, 0.08068109216841919), (210, 0.10997739313905487), (225, 0.016125794129544953), (254, 0.04185089721229258), (274, 0.045280672961440026), (278, 0.07945161777257916), (351, 0.03860218098862748), (393, 0.04261665742505229), (397, 0.06539403011355686), (433, 0.14846168836052567), (437, 0.060568827866825485), (457, 0.03815734324231499), (490, 0.039460953040126645), (499, 0.04778419034445287), (501, 0.031689640794038396), (519, 0.06606495240923384), (531, 0.04252023729317975), (539, 0.03959293098379113), (546, 0.21322508311996033), (568, 0.04929276253112153), (570, 0.04294810110450196), (580, 0.04302296580258748), (594, 0.04472568948137836), (599, 0.03631854622662227), (624, 0.1625045455101101), (643, 0.03984617867194187), (657, 0.09552086183787675), (658, 0.059028516009391595), (689, 0.0316015662084252), (803, 0.03599658613357395), (904, 0.10399675361514814), (1005, 0.23217908481903352), (1014, 0.08316858582076969), (1064, 0.029173566094507327), (1146, 0.04600314565908216), (1282, 0.05967645174297154), (1336, 0.08344244318275398), (1358, 0.059928616613415676), (1596, 0.05738540083931626), (1670, 0.32786876153824035), (1731, 0.05315296443613298), (1826, 0.055552156738573534), (1999, 0.09864002505404652), (2389, 0.060951408636175514), (2617, 0.07580141186234686), (2696, 0.05688049960159829), (2708, 0.06515902700579146), (2728, 0.15795165056289168), (2797, 0.11234020014787069), (2984, 0.07112477153743596), (2992, 0.08307586652342779), (3015, 0.0646134914712864), (3130, 0.09562036710590936), (3147, 0.06358887717729875), (3212, 0.09228397300954792), (3281, 0.04080866906224951), (3319, 0.08628636424785098), (3525, 0.03084532177591032), (3712, 0.41027385083730006), (3804, 0.0890134668648974), (4094, 0.10659880894386835), (4389, 0.099079485323982), (4464, 0.10091572695263566), (5277, 0.06622012086464997), (5987, 0.10984473242081066), (6829, 0.08868177427955615), (7364, 0.4033554286126997), (7805, 0.12310965234406572), (9438, 0.1445177265335158), (11614, 0.12217036358637708), (11813, 0.15036570533917834)]\n",
      "\n",
      "\n",
      "lda prediction:\n",
      "\n",
      "Score: 0.5141667127609253\t \n",
      "Topic: 0.011*\"trump\" + 0.009*\"like\" + 0.009*\"peopl\" + 0.007*\"presid\" + 0.007*\"obama\" + 0.007*\"know\" + 0.006*\"go\" + 0.006*\"american\" + 0.006*\"time\" + 0.005*\"think\"\n",
      "\n",
      "Score: 0.3785208463668823\t \n",
      "Topic: 0.071*\"trump\" + 0.017*\"republican\" + 0.015*\"donald\" + 0.010*\"presid\" + 0.010*\"campaign\" + 0.009*\"support\" + 0.007*\"go\" + 0.007*\"elect\" + 0.007*\"like\" + 0.006*\"peopl\"\n",
      "\n",
      "Score: 0.10066413879394531\t \n",
      "Topic: 0.034*\"trump\" + 0.014*\"presid\" + 0.013*\"white\" + 0.010*\"news\" + 0.009*\"twitter\" + 0.008*\"hous\" + 0.008*\"report\" + 0.007*\"black\" + 0.007*\"media\" + 0.006*\"http\"\n",
      "\n",
      "\n",
      " ['hous', 'card', 'star', 'kevin', 'spacey', 'play', 'fiction', 'charact', 'presid', 'frank', 'underwood', 'cnbc', 'intern', 'thursday', 'couldn', 'avoid', 'get', 'ask', 'question', 'concern', 'presidenti', 'elect', 'chief', 'question', 'happen', 'donald', 'trump', 'debat', 'frank', 'underwood', 'answer', 'give', 'draw', 'laughter', 'wouldn', 'debat', 'underwood', 'terribl', 'accid', 'debat', 'terribl', 'wasn', 'funni', 'follow', 'best', 'trump', 'imperson', 'huuug', 'huge', 'church', 'spacey', 'current', 'make', 'round', 'globe', 'promot', 'upcom', 'season', 'hous', 'card', 'air', 'march', 'ask', 'question', 'annual', 'skybridg', 'capit', 'recept', 'host', 'piano', 'earlier', 'say', 'get', 'ask', 'question', 'happen', 'frank', 'underwood', 'debat', 'trump', 'think', 'say', 'rememb', 'import', 'distinct', 'charact', 'fiction', 'charact', 'fiction', 'charact', 'turn', 'line', 'past', 'probabl', 'best', 'elabor', 'answer', 'provid', 'session', 'septemb', 'think', 'underwood', 'kill', 'donald', 'trump', 'trump', 'elect', 'think', 'great', 'donald', 'trump', 'start', 'feud', 'fiction', 'charact', 'mayb', 'fair', 'fight', 'trump', 'appear', 'season', 'hous', 'card', 'year', 'lose', 'real', 'elect', 'novemb', 'person', 'fetch', 'trump', 'rival', 'frank', 'underwood', 'featur', 'imag', 'screen', 'captur'] \n",
      "\n",
      " House of Cards star Kevin Spacey, who plays the fictional character President Frank Underwood, sat down with CNBC International on Thursday and just couldn t avoid getting asked questions concerning the 2016 presidential election. Chief among them was a question about what would happen if Donald Trump ever had to debate Frank Underwood.Here is the answer he gave, which drew quite a bit of laughter: He wouldn t (debate Underwood). There would be a terrible accident, on the way to the debate. It would be terrible, and very sad. If that wasn t funny enough, he followed that up with his best Trump impersonation: I m huuuge. I m too huge for this church. Spacey is currently making rounds across the globe promoting the new upcoming season of House of Cards, which airs in March. He was also asked this very same question at the annual SkyBridge Capital reception hosted at a piano bar just a day earlier. He says he often gets asked the question of what would happen if Frank Underwood had to debate Trump,  who would win? I thought about this and I said,  We must remember one important distinction, one of these characters is a fictional character, and the other is a fictional character.' It turns out he s used this line in the past often. Probably the best and most elaborate answer he s ever provided is from a Q & A session with CNN he did back in September. Oh, I think Underwood (would win). He (would win) because he would kill Donald Trump. Trump would never make it to election day. It d be over, done.I think it would be great if Donald Trump started a feud with a fictional character, because maybe for once it would be a fair fight. Perhaps Trump will appear in season 5 of House of Cards next year after he loses the real U.S. election this November. He is, after all, a TV personality. It s not too far-fetched that Trump could be a rival of Frank Underwood one day.Featured image via screen capture.\n"
     ]
    }
   ],
   "source": [
    "article = 30000\n",
    "\n",
    "print(\"\\nmost frequently occuring word: \", max(preprocess(corpus_texts[article]),key=preprocess(corpus_texts[article]).count))\n",
    "\n",
    "print(\"\\n\\ntf-idf representation:\")\n",
    "\n",
    "print(tfidf_model[bow_corpus_texts[article]])\n",
    "\n",
    "print(\"\\n\\nlda prediction:\")\n",
    "\n",
    "for index, score in sorted(lda_model[bow_corpus_texts[article]],\n",
    "                           key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))\n",
    "    \n",
    "print(\"\\n\\n\", preprocess(corpus_texts[article]), \"\\n\\n\", corpus_texts[article])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## X Saving items\n",
    "because we dont want to sit waiting every time please"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (0):\n",
    "    lda_modelXX.save(os.path.join(\"gensim\", \"lda_modelXX\"))\n",
    "    with open(os.path.join(\"gensim\", \"bag_of_words.pickle\"), 'wb') as f: pickle.dump(bow_corpus_texts, f)    \n",
    "    dictionary.save(os.path.join(\"gensim\", \"dictionary\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 10: Performing sentiment analysis <a class=\"anchor\" id=\"chapter3\"></a>\n",
    "Which we can do either to the whole article or on a sentence basis and then average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(text: list, method='VADER'):\n",
    "    if method == 'VADER':\n",
    "        scores = text.apply(lambda s: sia.polarity_scores(s)['compound']) #list of compound score per sentence\n",
    "    else:\n",
    "        scores = None\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9153 \n",
      "\n",
      " 0.10535714285714284 \n",
      "\n",
      "from:\n",
      " 0     0.4588\n",
      "1    -0.3818\n",
      "2    -0.3818\n",
      "3     0.4019\n",
      "4     0.0000\n",
      "5     0.0000\n",
      "6     0.1779\n",
      "7     0.1531\n",
      "8     0.8442\n",
      "9     0.4215\n",
      "10   -0.2960\n",
      "11    0.0000\n",
      "12    0.0772\n",
      "13    0.0000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "article = 10005\n",
    "\n",
    "sample_article = corpus_texts[article]\n",
    "sample_sentences = split_sentences(sample_article)\n",
    "\n",
    "sample_scoreT = sia.polarity_scores(sample_article)['compound']\n",
    "sample_scoreS = get_scores(sample_sentences)\n",
    "\n",
    "print(sample_scoreT, \"\\n\\n\", np.average(sample_scoreS), \"\\n\\nfrom:\\n\", sample_scoreS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_notebook",
   "language": "python",
   "name": "thesis_notebook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
