{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\emiel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\emiel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\emiel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "sia = SIA()\n",
    "\n",
    "import gensim\n",
    "\n",
    "import nltk\n",
    "#file for punkt splitter\n",
    "nltk.download('punkt');\n",
    "#file for vader sentiment\n",
    "nltk.download('vader_lexicon');\n",
    "\n",
    "#wordnet lemmatization\n",
    "nltk.download('wordnet')\n",
    "#more for preprocessing\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"]=20,20\n",
    "%matplotlib inline\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Masking, Embedding\n",
    "\n",
    "import glob\n",
    "import json\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# Topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "num_topics = 15\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary.load(os.path.join(\"gensim\", \"dictionary\"))\n",
    "\n",
    "bigram = gensim.models.phrases.Phrases.load(os.path.join('gensim', 'bigram.pkl'))\n",
    "trigram = gensim.models.phrases.Phrases.load(os.path.join('gensim', 'trigram.pkl'))\n",
    "\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "tfidf_model = gensim.models.TfidfModel.load(os.path.join('gensim', 'tfidf_model.pkl'))\n",
    "\n",
    "lda_tfidf_model = gensim.models.LdaMulticore.load(os.path.join(\"gensim\", \"02\", \"lda_model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.0630640747973534), (18, 0.020609302817723104), (19, 0.15387661217226292), (50, 0.04488019472778492), (52, 0.044539876958436406), (60, 0.03410322337525922), (61, 0.03209961257312386), (76, 0.019759801396857915), (90, 0.05563363852409356), (102, 0.04573375458652516), (106, 0.04180469038753974), (110, 0.20559324917334365), (124, 0.023293427382694194), (125, 0.04998384880175368), (138, 0.026169239538019667), (140, 0.043065346061500934), (150, 0.03097346697214275), (155, 0.08641863923306929), (169, 0.09605100049965742), (173, 0.032832981567750864), (180, 0.0423381198999444), (181, 0.020641173417236), (186, 0.023891827406085326), (189, 0.20196668286594258), (195, 0.0663901160677921), (196, 0.03445425715814743), (197, 0.03384255408349029), (201, 0.030139608848495487), (205, 0.014156029831098978), (216, 0.08138936610877293), (225, 0.0573063845155341), (278, 0.029238562700818666), (281, 0.032007013860166744), (297, 0.049677727337640566), (309, 0.04849918971288157), (316, 0.04097534409272348), (327, 0.0440378894572151), (329, 0.017222095697276243), (338, 0.046769402853804064), (340, 0.03602117813462508), (341, 0.1828452903314407), (342, 0.08651053859940701), (343, 0.07277194677459341), (344, 0.08006165553237464), (345, 0.13159873536471897), (346, 0.04525243769560385), (347, 0.04214178404500105), (348, 0.08957449210571236), (349, 0.051738822020007395), (350, 0.06385396698586593), (351, 0.05383053522501099), (352, 0.12911501327403235), (353, 0.07162155067638407), (354, 0.08452959727038784), (355, 0.05209448288910188), (356, 0.2751596194084071), (357, 0.04338473761366265), (358, 0.03485400398308257), (359, 0.06775338979024405), (360, 0.034742176723649036), (361, 0.06276159241175783), (362, 0.07875149431897374), (363, 0.04382519396127777), (364, 0.10263642040570053), (365, 0.05841722274887979), (366, 0.12224571736162554), (367, 0.25897411143458904), (368, 0.0990180974850954), (369, 0.055623575103368295), (370, 0.05202112414343383), (371, 0.07492383447189768), (372, 0.051875352771229635), (373, 0.03849624542650644), (374, 0.15463362615359721), (375, 0.08457054162002885), (376, 0.13994318700266095), (377, 0.0363469299923152), (378, 0.20942563165373543), (379, 0.09825367752530083), (380, 0.05014447223697731), (381, 0.09812956145155674), (382, 0.05934009138074462), (383, 0.09304067950746954), (384, 0.04000368392934746), (385, 0.257516655158138), (386, 0.21712852955154796), (387, 0.05869231574721387), (388, 0.02641244770355307), (389, 0.10682512188338879), (390, 0.04996076175592935), (391, 0.035501002094633416), (392, 0.03246232915452445), (393, 0.12378978649385201), (394, 0.06430135507965806), (395, 0.058334377751070116), (396, 0.1029148926477512), (397, 0.12564543378731183), (398, 0.01887137146621889), (399, 0.05943171024938987), (400, 0.10811245751293135), (401, 0.10487564396056057), (402, 0.046805092877381396), (403, 0.23457669366080872), (404, 0.10094851472529107), (405, 0.04662669423042824), (406, 0.04441182590931878), (407, 0.058886353868783346), (408, 0.03863592666798838), (409, 0.13798300086436557), (410, 0.10724345840444015), (411, 0.06457215834054607), (412, 0.0967070394369872), (413, 0.11683444549775958)]\n"
     ]
    }
   ],
   "source": [
    "def stem_lemmatize(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def make_ngrams(text):\n",
    "    return trigram_mod[bigram_mod[text]]\n",
    "\n",
    "def topic_preprocess(text):\n",
    "    #all the nice preprocessing without the bigrams and trigrams\n",
    "    output = []\n",
    "    \n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            output.append(stem_lemmatize(token))\n",
    "            \n",
    "    output = make_ngrams(output)\n",
    "    \n",
    "    output = dictionary.doc2bow(output)\n",
    "    \n",
    "    return tfidf_model[output]\n",
    "\n",
    "text = 'WASHINGTON (Reuters) - The special counsel investigation of links between Russia and President Trump’s 2016 election campaign should continue without interference in 2018, despite calls from some Trump administration allies and Republican lawmakers to shut it down, a prominent Republican senator said on Sunday. Lindsey Graham, who serves on the Senate armed forces and judiciary committees, said Department of Justice Special Counsel Robert Mueller needs to carry on with his Russia investigation without political interference. “This investigation will go forward. It will be an investigation conducted without political influence,” Graham said on CBS’s Face the Nation news program. “And we all need to let Mr. Mueller do his job. I think he’s the right guy at the right time.”  The question of how Russia may have interfered in the election, and how Trump’s campaign may have had links with or co-ordinated any such effort, has loomed over the White House since Trump took office in January. It shows no sign of receding as Trump prepares for his second year in power, despite intensified rhetoric from some Trump allies in recent weeks accusing Mueller’s team of bias against the Republican president. Trump himself seemed to undercut his supporters in an interview last week with the New York Times in which he said he expected Mueller was “going to be fair.”    Russia’s role in the election and the question of possible links to the Trump campaign are the focus of multiple inquiries in Washington. Three committees of the Senate and the House of Representatives are investigating, as well as Mueller, whose team in May took over an earlier probe launched by the U.S. Federal Bureau of Investigation (FBI). Several members of the Trump campaign and administration have been convicted or indicted in the investigation.  Trump and his allies deny any collusion with Russia during the campaign, and the Kremlin has denied meddling in the election. Graham said he still wants an examination of the FBI’s use of a dossier on links between Trump and Russia that was compiled by a former British spy, Christopher Steele, which prompted Trump allies and some Republicans to question Mueller’s inquiry.   On Saturday, the New York Times reported that it was not that dossier that triggered an early FBI probe, but a tip from former Trump campaign foreign policy adviser George Papadopoulos to an Australian diplomat that Russia had damaging information about former Trump rival Hillary Clinton.  “I want somebody to look at the way the Department of Justice used this dossier. It bothers me greatly the way they used it, and I want somebody to look at it,” Graham said. But he said the Russia investigation must continue. “As a matter of fact, it would hurt us if we ignored it,” he said. ' \n",
    "\n",
    "print(topic_preprocess(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, 0.54721564), (13, 0.3566922)]\n"
     ]
    }
   ],
   "source": [
    "def doc_to_topics(text):\n",
    "    return lda_tfidf_model[topic_preprocess(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "\n",
    "# LSTM +  Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_model = keras.models.load_model(os.path.join('keras', 'LSTM_tree'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LSTM_pred': 0.617904543876648, 'count': 17.0, 'mean': -0.01017647058823531, 'std': 0.45047517208139753, 'min': -0.6808, '25%': -0.2732, '50%': 0.0, '75%': 0.3818, 'max': 0.7269}\n"
     ]
    }
   ],
   "source": [
    "series_len = 100\n",
    "mask_value = -10\n",
    "max_len = 0\n",
    "\n",
    "num_topics = 15\n",
    "\n",
    "def split_sentences(article_text):\n",
    "    \"\"\"Takes a string, returns a list of its individual sentences ()\"\"\"\n",
    "    sentence_list = nltk.tokenize.sent_tokenize(article_text)\n",
    "    return pd.Series(sentence_list)\n",
    "\n",
    "def sentences_to_scores(text: list, method='VADER'):\n",
    "    \"\"\"Takes a list of sentences, returns a list of sentiment scores (per sentence)\"\"\"\n",
    "    if method == 'VADER':\n",
    "        scores = text.apply(lambda s: sia.polarity_scores(s)['compound']) #list of compound score per sentence\n",
    "    else:\n",
    "        scores = None\n",
    "    return scores\n",
    "\n",
    "def scores_to_sequence(scores):\n",
    "    if len(scores) < series_len:\n",
    "        t  = series_len - len(scores)\n",
    "        scores = np.pad(scores, (t, 0), mode='constant', constant_values=mask_value)\n",
    "        scores = scores.reshape((series_len, 1))\n",
    "    else:\n",
    "        scores = np.array(scores[(-1*series_len):])\n",
    "        scores = scores.reshape((series_len, 1))\n",
    "    return scores\n",
    "\n",
    "def doc_to_stats(doc):\n",
    "    \"\"\"takes a text, returns a dict with the numpy descibe of the sentiment stats and LSTM prediction\"\"\"\n",
    "    scores = sentences_to_scores(split_sentences(doc))\n",
    "    sequence = scores_to_sequence(scores)\n",
    "    output = {'LSTM_pred' : float(LSTM_model(np.array([sequence])))}\n",
    "    output.update(scores.describe().to_dict())\n",
    "    return (output)\n",
    "\n",
    "\n",
    "print(doc_to_stats(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# combination definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'T0': 0, 'T1': 0, 'T2': 0, 'T3': 0, 'T4': 0, 'T5': 0.5471641, 'T6': 0, 'T7': 0, 'T8': 0, 'T9': 0, 'T10': 0, 'T11': 0, 'T12': 0, 'T13': 0.3567433, 'T14': 0, 'LSTM_pred': 0.617904543876648, 'count': 17.0, 'mean': -0.01017647058823531, 'std': 0.45047517208139753, 'min': -0.6808, '25%': -0.2732, '50%': 0.0, '75%': 0.3818, 'max': 0.7269, 'veracity': 1}\n",
      "Wall time: 355 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "stat_cols = ['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max', 'LSTM_pred']\n",
    "topic_cols = [('T'+str(x)) for x in range(0, num_topics)]\n",
    "\n",
    "# text to list of topic scores\n",
    "\n",
    "def doc_to_row(doc, veracity):\n",
    "    new_row = dict.fromkeys(topic_cols , 0)\n",
    "    \n",
    "    new_row.update(doc_to_stats(doc)) # add the sentiment key and value pairs\n",
    "    \n",
    "    for index, score in doc_to_topics(doc): # update the topic distributions\n",
    "        new_row['T'+str(index)] = score\n",
    "\n",
    "    new_row['veracity'] = veracity\n",
    "        \n",
    "    return(new_row)\n",
    "\n",
    "print(doc_to_row(text, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# Applying to the new Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>LSTM_pred</th>\n",
       "      <th>T0</th>\n",
       "      <th>...</th>\n",
       "      <th>T6</th>\n",
       "      <th>T7</th>\n",
       "      <th>T8</th>\n",
       "      <th>T9</th>\n",
       "      <th>T10</th>\n",
       "      <th>T11</th>\n",
       "      <th>T12</th>\n",
       "      <th>T13</th>\n",
       "      <th>T14</th>\n",
       "      <th>veracity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30.0</td>\n",
       "      <td>-0.186697</td>\n",
       "      <td>0.581253</td>\n",
       "      <td>-0.9360</td>\n",
       "      <td>-0.690150</td>\n",
       "      <td>-0.1217</td>\n",
       "      <td>0.245100</td>\n",
       "      <td>0.9041</td>\n",
       "      <td>0.508691</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0319098</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.850386</td>\n",
       "      <td>0.0506049</td>\n",
       "      <td>TRUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24.0</td>\n",
       "      <td>0.274229</td>\n",
       "      <td>0.478666</td>\n",
       "      <td>-0.7003</td>\n",
       "      <td>-0.007150</td>\n",
       "      <td>0.4215</td>\n",
       "      <td>0.653100</td>\n",
       "      <td>0.8750</td>\n",
       "      <td>0.787888</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0504311</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.781340</td>\n",
       "      <td>0</td>\n",
       "      <td>TRUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54.0</td>\n",
       "      <td>0.175761</td>\n",
       "      <td>0.486247</td>\n",
       "      <td>-0.8807</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1492</td>\n",
       "      <td>0.621775</td>\n",
       "      <td>0.9153</td>\n",
       "      <td>0.418227</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0316043</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.841872</td>\n",
       "      <td>0.0717724</td>\n",
       "      <td>TRUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>-0.168044</td>\n",
       "      <td>0.600156</td>\n",
       "      <td>-0.9231</td>\n",
       "      <td>-0.603800</td>\n",
       "      <td>-0.4404</td>\n",
       "      <td>0.288900</td>\n",
       "      <td>0.9552</td>\n",
       "      <td>0.368838</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0579058</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.834327</td>\n",
       "      <td>0</td>\n",
       "      <td>TRUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27.0</td>\n",
       "      <td>-0.195730</td>\n",
       "      <td>0.504106</td>\n",
       "      <td>-0.8750</td>\n",
       "      <td>-0.618150</td>\n",
       "      <td>-0.1027</td>\n",
       "      <td>0.036900</td>\n",
       "      <td>0.8658</td>\n",
       "      <td>0.531730</td>\n",
       "      <td>0.0488056</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0375881</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0475887</td>\n",
       "      <td>0.0228834</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.770457</td>\n",
       "      <td>0</td>\n",
       "      <td>TRUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.326400</td>\n",
       "      <td>0.547950</td>\n",
       "      <td>-0.7430</td>\n",
       "      <td>0.327175</td>\n",
       "      <td>0.5103</td>\n",
       "      <td>0.649775</td>\n",
       "      <td>0.7184</td>\n",
       "      <td>0.341741</td>\n",
       "      <td>0.0658839</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0918253</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.713498</td>\n",
       "      <td>0.0289415</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>21.0</td>\n",
       "      <td>-0.231776</td>\n",
       "      <td>0.322246</td>\n",
       "      <td>-0.8622</td>\n",
       "      <td>-0.510600</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2023</td>\n",
       "      <td>0.514779</td>\n",
       "      <td>0.051523</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.117269</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.698151</td>\n",
       "      <td>0</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>11.0</td>\n",
       "      <td>0.084145</td>\n",
       "      <td>0.504422</td>\n",
       "      <td>-0.7351</td>\n",
       "      <td>-0.318000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.513400</td>\n",
       "      <td>0.7269</td>\n",
       "      <td>0.359130</td>\n",
       "      <td>0.113842</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0100423</td>\n",
       "      <td>0.0100423</td>\n",
       "      <td>0.0100435</td>\n",
       "      <td>0.0100423</td>\n",
       "      <td>0.0100423</td>\n",
       "      <td>0.0100423</td>\n",
       "      <td>0.0321558</td>\n",
       "      <td>0.703802</td>\n",
       "      <td>0.0208956</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>12.0</td>\n",
       "      <td>-0.060133</td>\n",
       "      <td>0.320544</td>\n",
       "      <td>-0.6124</td>\n",
       "      <td>-0.311875</td>\n",
       "      <td>-0.0258</td>\n",
       "      <td>0.156575</td>\n",
       "      <td>0.4019</td>\n",
       "      <td>0.588307</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0356699</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.546104</td>\n",
       "      <td>0</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>14.0</td>\n",
       "      <td>-0.087464</td>\n",
       "      <td>0.322474</td>\n",
       "      <td>-0.6310</td>\n",
       "      <td>-0.219500</td>\n",
       "      <td>-0.0129</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.4019</td>\n",
       "      <td>0.593341</td>\n",
       "      <td>0.0253816</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0474805</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0348409</td>\n",
       "      <td>0.748637</td>\n",
       "      <td>0</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>422 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     count      mean       std     min       25%     50%       75%     max  \\\n",
       "0     30.0 -0.186697  0.581253 -0.9360 -0.690150 -0.1217  0.245100  0.9041   \n",
       "1     24.0  0.274229  0.478666 -0.7003 -0.007150  0.4215  0.653100  0.8750   \n",
       "2     54.0  0.175761  0.486247 -0.8807  0.000000  0.1492  0.621775  0.9153   \n",
       "3     16.0 -0.168044  0.600156 -0.9231 -0.603800 -0.4404  0.288900  0.9552   \n",
       "4     27.0 -0.195730  0.504106 -0.8750 -0.618150 -0.1027  0.036900  0.8658   \n",
       "..     ...       ...       ...     ...       ...     ...       ...     ...   \n",
       "417    6.0  0.326400  0.547950 -0.7430  0.327175  0.5103  0.649775  0.7184   \n",
       "418   21.0 -0.231776  0.322246 -0.8622 -0.510600  0.0000  0.000000  0.2023   \n",
       "419   11.0  0.084145  0.504422 -0.7351 -0.318000  0.0000  0.513400  0.7269   \n",
       "420   12.0 -0.060133  0.320544 -0.6124 -0.311875 -0.0258  0.156575  0.4019   \n",
       "421   14.0 -0.087464  0.322474 -0.6310 -0.219500 -0.0129  0.000000  0.4019   \n",
       "\n",
       "     LSTM_pred         T0  ...         T6         T7         T8         T9  \\\n",
       "0     0.508691          0  ...          0          0  0.0319098          0   \n",
       "1     0.787888          0  ...          0          0          0  0.0504311   \n",
       "2     0.418227          0  ...          0          0  0.0316043          0   \n",
       "3     0.368838          0  ...          0          0  0.0579058          0   \n",
       "4     0.531730  0.0488056  ...          0  0.0375881          0  0.0475887   \n",
       "..         ...        ...  ...        ...        ...        ...        ...   \n",
       "417   0.341741  0.0658839  ...          0          0  0.0918253          0   \n",
       "418   0.514779   0.051523  ...          0          0   0.117269          0   \n",
       "419   0.359130   0.113842  ...  0.0100423  0.0100423  0.0100435  0.0100423   \n",
       "420   0.588307          0  ...  0.0356699          0          0          0   \n",
       "421   0.593341  0.0253816  ...          0          0  0.0474805          0   \n",
       "\n",
       "           T10        T11        T12       T13        T14 veracity  \n",
       "0            0          0          0  0.850386  0.0506049     TRUE  \n",
       "1            0          0          0  0.781340          0     TRUE  \n",
       "2            0          0          0  0.841872  0.0717724     TRUE  \n",
       "3            0          0          0  0.834327          0     TRUE  \n",
       "4    0.0228834          0          0  0.770457          0     TRUE  \n",
       "..         ...        ...        ...       ...        ...      ...  \n",
       "417          0          0          0  0.713498  0.0289415     FAKE  \n",
       "418          0          0          0  0.698151          0     FAKE  \n",
       "419  0.0100423  0.0100423  0.0321558  0.703802  0.0208956     FAKE  \n",
       "420          0          0          0  0.546104          0     FAKE  \n",
       "421          0          0  0.0348409  0.748637          0     FAKE  \n",
       "\n",
       "[422 rows x 25 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cols = ['file', 'text', 'veracity', 'prediction_emiel']\n",
    "\n",
    "p_RealBuzz = os.path.join('sources', 'akkerman', 'RealBuzzfeed')\n",
    "p_RealPoli = os.path.join('sources', 'akkerman', 'RealPolitifact')\n",
    "p_FakeBuzz = os.path.join('sources', 'akkerman', 'FakeBuzzfeed')\n",
    "p_FakePoli = os.path.join('sources', 'akkerman', 'FakePolitifact')\n",
    "\n",
    "directories = [[p_RealBuzz, 'TRUE'],\n",
    "               [p_RealPoli, 'TRUE'],\n",
    "               [p_FakeBuzz, 'FAKE'],\n",
    "               [p_FakePoli, 'FAKE']]\n",
    "\n",
    "df_Complete = pd.DataFrame()\n",
    "\n",
    "for path, veracity in directories:\n",
    "    df_part = None\n",
    "    df_part = pd.DataFrame(columns=stat_cols+topic_cols+['veracity'])\n",
    "    \n",
    "    for filename in glob.glob(os.path.join(path, '*.json')): # loop over .json files in the cwd\n",
    "        with open(filename) as f:\n",
    "            new_row = doc_to_row(json.load(f)['text'], veracity)\n",
    "            df_part = df_part.append(new_row, ignore_index=True)\n",
    "      \n",
    "    df_Complete = df_Complete.append(df_part, ignore_index=True)\n",
    "\n",
    "display(df_Complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Complete.to_csv(os.path.join('out','AkkermanData_Emiel.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_notebook",
   "language": "python",
   "name": "thesis_notebook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
