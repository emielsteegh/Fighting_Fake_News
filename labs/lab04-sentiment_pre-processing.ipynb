{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing\n",
    "\n",
    "In this notebook we will use what we learned in lab 03 about forming the data, the end result is a dataset that we can feed weka to test models on.\n",
    "\n",
    "### So we will:\n",
    "\n",
    "1. [Load the dataset](#chapter1)\n",
    "2. [Split the text into sentences](#chapter2)\n",
    "3. [Perform sentiment analysis on the sentences](#chapter3)\n",
    "4. [Include statistical features](#chapter4)\n",
    "5. [Resample the data to 150 points](#chapter5)\n",
    "6. [Store it in a row of a dataframe](#chapter6)\n",
    "7. [Export it to a csv](#chapter7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "sia = SIA()\n",
    "\n",
    "import nltk\n",
    "#file for punkt splitter\n",
    "nltk.download('punkt');\n",
    "#file for vader sentiment\n",
    "nltk.download('vader_lexicon');\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"]=20,20\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 1: Load the dataset <a class=\"anchor\" id=\"chapter1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path_true = os.path.join(\"sources\", \"ISOT\", \"True.csv\")\n",
    "dataset_path_fake = os.path.join(\"sources\", \"ISOT\", \"Fake.csv\")\n",
    "\n",
    "dataset_load_true = pd.read_csv(dataset_path_true, encoding='utf-8') # make sure to use the right encoding\n",
    "dataset_load_fake = pd.read_csv(dataset_path_fake, encoding='utf-8') \n",
    "\n",
    "display(dataset_load_true.head())\n",
    "display(dataset_load_fake.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 2: Split the text into sentences <a class=\"anchor\" id=\"chapter2\"></a>\n",
    "\n",
    "in lab 03 we used a big manual function, but for now we will use the nltk tokenizer in this one :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentences(article_text):\n",
    "    \"\"\"Takes a string, returns a list of its individual sentences ()\"\"\"\n",
    "    return pd.Series(nltk.tokenize.sent_tokenize(article_text))\n",
    "\n",
    "sample_sentences = split_sentences(dataset_load_true.text[0])\n",
    "display(sample_sentences.iloc[[0,1,-2,-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 3: Performing sentiment analysis <a class=\"anchor\" id=\"chapter3\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(text: list, method='VADER'):\n",
    "    if method == 'VADER':\n",
    "        scores = text.apply(lambda s: sia.polarity_scores(s)['compound']) #list of compound score per sentence\n",
    "    else:\n",
    "        scores = None\n",
    "\n",
    "    return scores\n",
    "\n",
    "sample_scores = get_scores(sample_sentences)\n",
    "print(sample_scores.iloc[[0,1,-2,-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 4: Include statistical features <a class=\"anchor\" id=\"chapter4\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(scores):\n",
    "    stats = scores.describe()\n",
    "    return stats\n",
    "\n",
    "sample_stats = get_stats(sample_scores)\n",
    "print(sample_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 5: Resample the data <a class=\"anchor\" id=\"chapter5\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_to_x(scores, scores_len, x = 200, to_dict = False):\n",
    "    if scores_len <= x: #resample to higher (or same) res\n",
    "        if scores_len > 1:\n",
    "            slope = (x - 1)/ (scores_len - 1)\n",
    "            new_indices = ['p_'+f'{int(slope * i):03}' for i in range(int(scores_len))]\n",
    "\n",
    "            new_scores = scores[:]\n",
    "            new_scores.index = new_indices\n",
    "\n",
    "            new_series = pd.Series(index = ['p_'+f'{i:03}' for i in range(int(x))], dtype='float64')\n",
    "\n",
    "            output = new_scores.combine_first(new_series)\n",
    "            output = output.interpolate()\n",
    "        elif scores_len == 1:\n",
    "            output = pd.Series(data = scores[0], index = ['p_'+f'{i:03}' for i in range(int(x))], dtype='float64')\n",
    "        else:\n",
    "            output = pd.Series(data = None, index = ['p_'+f'{i:03}' for i in range(int(x))], dtype='float64')\n",
    "            \n",
    "    elif scores_len > x: #downscale ;(\n",
    "        y = int(np.ceil(len(scores)/x)) # bin size \n",
    "        \n",
    "        new_scores = [np.mean(i) for i in np.array_split(np.array(scores), x)]\n",
    "        output = pd.Series(data = new_scores, index = ['p_'+f'{i:03}' for i in range(int(x))], dtype='float64')\n",
    "        \n",
    "    if not to_dict:\n",
    "        return output\n",
    "    else:\n",
    "        return output.to_dict()\n",
    "    \n",
    "sample_interp = resample_to_x(sample_scores, sample_stats['count'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=1)\n",
    "plt.subplots_adjust(hspace=0.8)\n",
    "\n",
    "sample_scores.plot(ax=axes[0]);\n",
    "axes[0].set_title('Original Scores');\n",
    "\n",
    "sample_interp.plot(ax=axes[1]);\n",
    "axes[1].set_title('Resampled Scores');\n",
    "\n",
    "downsample_interp = resample_to_x(sample_scores, sample_stats['count'], x=10)\n",
    "downsample_interp.plot(ax=axes[2]);\n",
    "axes[2].set_title('Downsampled Scores');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 6: Create the dataframe <a class=\"anchor\" id=\"chapter6\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all(text, veracity: int, resample_size: int = 200):\n",
    "    try: \n",
    "        # should all be dicts with the appropriate name\n",
    "        sentences = split_sentences(text) # list\n",
    "        scores = get_scores(sentences) #  list but ok, will be\n",
    "        stats = get_stats(scores) # dict yes!\n",
    "        scores_interp = resample_to_x(scores, stats['count'], resample_size, to_dict=True) # dict si (p_000 through p_resamplesize)\n",
    "        v = {'veracity': veracity}\n",
    "        v.update(stats)\n",
    "        v.update(scores_interp)\n",
    "        return pd.Series(v)\n",
    "    except Exception as e:\n",
    "        print(\"ERROR on text: \\n\"+text+\"\\n\\n\",stats['count'])\n",
    "        raise e\n",
    "\n",
    "# print(get_all(dataset_load_true.text[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# this cell is for verifying that the method at least works on a small scale (8 instances)\n",
    "# much quicker to debug than large scale (40k+ instances) ;(\n",
    "\n",
    "resample_size = 10\n",
    "\n",
    "cols = ['veracity', 'count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max']\n",
    "cols.extend(['p_'+f'{n:03}' for n in range(resample_size)])\n",
    "\n",
    "miniset_true = dataset_load_true.head()\n",
    "miniset_fake = dataset_load_fake.head()\n",
    "\n",
    "miniset_prepro = pd.DataFrame(columns = cols)\n",
    "miniset_prepro[cols] = miniset_true.text.apply(get_all, args=(1, resample_size));\n",
    "miniset_prepro = pd.concat([miniset_prepro, (miniset_fake.text.apply(get_all, args=(0, resample_size)))], ignore_index=True)\n",
    "\n",
    "miniset_prepro.iloc[[0,1,-2,-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "resample_size = 200\n",
    "\n",
    "cols = ['veracity', 'count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max']\n",
    "cols.extend(['p_'+f'{n:03}' for n in range(resample_size)])\n",
    "\n",
    "dataset_prepro = pd.DataFrame(columns = cols)\n",
    "\n",
    "print(\"We'll give it a shot!\")\n",
    "dataset_prepro[cols] = dataset_load_true.text.apply(get_all, args=(1, resample_size));\n",
    "print(\"Woah, we're halfway there!\")\n",
    "dataset_prepro = pd.concat([miniset_prepro, (dataset_load_fake.text.apply(get_all, args=(0, resample_size)))], ignore_index=True);\n",
    "print(\"Woah, livin' on a prayer\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_notebook",
   "language": "python",
   "name": "thesis_notebook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
